<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhenglun Kong</title>
  
  <meta name="author" content="Zhenglun Kong">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">

  <style type="text/css">

    .school-logo {
      width: 6%;
      float: left;
    }

    .school-text {
      margin-left: 10%;
      width: 60%;
    }
  </style>


</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zhenglun Kong</name>
              </p>
              <p>I am currently a postdoctoral researcher at Harvard, working closely with <a href="https://dbmi.hms.harvard.edu/people/marinka-zitnik">Marinka Zitnik</a> at Harvard and  <a href="https://web.mit.edu/manoli/">Manolis Kellis</a> at MIT. I received my PhD from Northeastern University in 2024, supervised by Prof. <a href="https://web.northeastern.edu/yanzhiwang/#_ga=2.59245165.1964588443.1663640196-2055581220.1641240155">Yanzhi Wang</a>. Prior to that, I earned my master degree from Northeastern University in 2019 and B.E. degree from Huazhong University of Science and Technology (HUST), China, in 2017. I was a research intern at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-redmond/">Microsoft Research</a>, <a href="https://careers.arm.com/usa-san-jose-office">ARM</a>, and <a href="https://sra.samsung.com/">Samsung Research</a>. My research focuses on developing efficient deep learning methods applicable to real-world scenarios, including computer vision and natural language processing. I was selected as the <a href="https://mlcommons.org/2024/06/2024-mlc-rising-stars">Machine Learning and Systems Rising Stars 2024</a>.
              </p>
<!--                   <p>
                  <b><font color="red">
                    I am currently looking for postdoctoral and faculty positions starting Fall 2024. Please kindly reach out to me for any opportunities. Thanks!</font>
                  </p> -->
              <p style="text-align:center">
                <a href="mailto:kong.zhe@northeastern.edu">Email</a> &nbsp/&nbsp
                <a href="https://drive.google.com/file/d/15FEinl3uXQaLz-zUP9iYjNMKMmcMkDqI/view?usp=sharing">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=XYa4NVYAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/ZLKong/">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/zhenglun-kong-35b527150/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://twitter.com/ZKong50693">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/profile_new.jpeg"><img style="width:80%;max-width:80%" alt="profile photo" src="images/profile_new.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
              I am dedicated to achieving the universality and practical implementation of AI. My research is particularly focused on the following domains and methodologies:
              <p>
            <ul>
           <li><p>
              <b>Efficient Deep Learning</b>:  Accelerating pre-training/fine-tuning and inference, data/model compression, and fast & robust DNN design for Large Language Models (GPTs, LLaMA, BERT, etc.) and Vision Models (ViTs, Diffusion, DETR, ResNet, etc.)
            </p>
<!--             <li><p> 
              <b>Tasks & Applications</b>: Image/text classification, autonomous driving systems, medical AI (image segmentation, fariness), synthetic data generation, and generative AI in business.
            </p> -->
           <li><p>
              <b>Methodologies</b>: Token/weight pruning, quantization, sparse training, data distillation, and latency-aware neural architecture search.
            </p>
            <li><p>
              <b>AI for Health</b>: Efficient algorithms for AI agents and cell foundation models. 
            </p>
            </ul>
          </td>
        </tr>

        <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle;background-color:#fffff0;">
              <heading>News</heading>
              <p></p >
              <div class="listbox">
                <ul>
                  <li>05/2024: Honored to be selected as the <a href="https://mlcommons.org/2024/06/2024-mlc-rising-stars">2024 Machine Learning and Systems Rising Stars</a>.
                  <li>04/2024: I will join <a href="https://kempnerinstitute.harvard.edu/">Harvard</a> as a Postdoctoral Research Fellow in August.
                  <li>01/2024: Our paper on Activation-Guided Quantization for Large Language Models has been accepted by <a href="https://aaai.org/aaai-conference/">AAAI'24</a>
                  <li>01/2024: I gave a talk at the Robotics Institute at <a href="https://www.ri.cmu.edu/">CMU</a> during the <a href="https://www.ri.cmu.edu/event/towards-energy-efficient-techniques-and-applications-for-universal-ai-implementation/">VASC Seminar</a>. The topic was "<i>Towards Efficient Techniques and Applications for Universal AI Implementation.</i>"
                  <li>09/2023: Our paper on Hardware-oriented 3D Detector is accepted by <a href="https://nips.cc/">NeurIPS'23</a>, see you in New Orleans!
              </ul>
              </div>
          </td>
        </tr>


        <table
          style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Experiences</heading>
                <p>
                <ul id="exp" style="list-style:none;">
                  <li style="margin-bottom: 20px;"> <img class="school-logo" src="images/harvard.png">
                    <div class="school-text">Harvard University, August 2024 - present <br />Postdoctoral researcher
                     <br /> Mentors: <a href="https://dbmi.hms.harvard.edu/people/marinka-zitnik">Marinka Zitnik</a>, <a href="https://web.mit.edu/manoli/">Manolis Kellis</a>
                    </div>
                  <li style="margin-bottom: 20px;"> <img class="school-logo" src="images/northeastern_logo.png">
                    <div class="school-text"> Northeastern University, September 2019 - July 2024,<br />Research Assistant
                      <br /> Advisor: Prof. <a href="https://web.northeastern.edu/yanzhiwang/#_ga=2.59245165.1964588443.1663640196-2055581220.1641240155">Yanzhi Wang</a>
                    </div>
                  </li>
                  <li style="margin-bottom: 20px;"> <img class="school-logo" src="images/Microsoft_logo.png">
                    <div class="school-text">Microsoft, June 2022 - August 2022 <br />Research Intern
                     <br /> Mentor: <a href="https://www.linkedin.com/in/subho87/">Subhabrata Mukherjee</a>  
                    </div>
                  <li style="margin-bottom: 20px;"> <img class="school-logo" src="images/Arm_logo_2017.png">
                    <div class="school-text">ARM, June 2021 - August 2021 <br />Research Intern
                     <br /> Mentors: <a href="https://www.linkedin.com/in/lcmeng">Lingchuan Meng</a>, <a href="https://www.zoominfo.com/p/Danny-Loh/1893030987">Danny Loh</a>
                    </div>
                  <li style="margin-bottom: 20px;"> <img class="school-logo" src="images/samsung.png">
                    <div class="school-text">Samsung Research America, October 2021 - December 2021 <br />Research Intern
                     <br /> Mentors: <a href="https://sites.google.com/site/avikdelta">Avik Ray</a>, <a href="https://www.linkedin.com/in/yilin-shen-65a56622/">Yilin Shen</a>, <a href="https://www.linkedin.com/in/hongxiajin/">Hongxia Jin</a>
                    </div>
                  </li>
                </ul>
                </p>
              </td>
            </tr>
          </tbody>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
            <tr>
              <td>
                <heading style="white-space: nowrap;">Selected Publications</heading> <br>
                <br> * means equal contribution
              </td>
            </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/QAT.png" alt="3DSP" width="180" height="135" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP" href="https://arxiv.org/pdf/2402.10787.pdf">
                <papertitle>EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the Edge</papertitle>
              </a>
              <br>
              Xuan Shen, <b>Zhenglun Kong</b>, Changdi Yang, Zhaoyang Han, Lei Lu, Peiyan Dong, Cheng Lyu, Chih-hsiang Li, Xuehang Guo, Zhihao Shu, Wei Niu, Miriam Leeser, Pu Zhao, Yanzhi Wang
              <br>
              <i> arXiv:2402.10787 </i>
              <br>
              <a href="https://arxiv.org/pdf/2402.10787.pdf"><font color="red">PDF</font></a> / 
              <a href="https://github.com/shawnricecake/EdgeQAT"><font color="red">Code</font></a>
              <p></p>
              <p> We design an entropy \& distribution guided quantization method to reduce information distortion in quantized query, key, and attention maps, tackling the bottleneck of QAT for LLMs. </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/AAAI2024.png" alt="3DSP" width="180" height="135" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP" href="https://arxiv.org/pdf/2312.05693.pdf">
                <papertitle>Agile-Quant: Activation-Guided Quantization for Faster Inference of LLMs on the Edge</papertitle>
              </a>
              <br>
              Xuan Shen, Peiyan Dong, Lei Lu, <b>Zhenglun Kong</b>, Zhengang Li, Ming Lin, Chao Wu, Yanzhi Wang
              <br>
              <em><b>[AAAI 2024]</b></em> <i>The Thirty-Seventh AAAI Conference on Artificial Intelligence</i>
              <br>
              <a href="https://arxiv.org/pdf/2312.05693.pdf"><font color="red">PDF</font></a> 
              <p></p>
              <p> We propose an activation-guided quantization framework for popular Large Language Models (LLMs). Specifically, with 4-bit or 8-bit for the activation and 4-bit for the weight quantization. </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/TMLR.png" alt="3DSP" width="180" height="135" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP" href="https://openreview.net/pdf?id=sTdd0yCOZ2">
                <papertitle>Lightweight Vision Transformer Coarse-to-Fine Search via Latency Profiling</papertitle>
              </a>
              <br>
              <b>Zhenglun Kong</b>, Dongkuan Xu, Zhengang Li, Peiyan Dong, Hao Tang, Yanzhi Wang, Subhabrata Mukherjee
              <br>
              <em><b>[TMLR]</b></em> <i>Transactions on Machine Learning Research</i>
              <br>
              <a href="https://openreview.net/pdf?id=sTdd0yCOZ2"><font color="red">PDF</font></a> 
              <p></p>
              <p> We introduce a truly efficient, hardware-oriented approach for searching efficient vision transformer structure. This approach has been optimized to seamlessly adapt to the constraints of the target hardware and fulfill the specific speed requirements.</p>
            </td>
          </tr>
          <tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Neurips2023.png" alt="3DSP" width="180" height="135" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP"  href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=XYa4NVYAAAAJ&sortby=pubdate&citation_for_view=XYa4NVYAAAAJ:QIV2ME_5wuYC">
                <papertitle>HotBEV: Hardware-oriented Transformer-based Multi-View 3D Detector for BEV Perception</papertitle>
              </a>
              <br>
              <b>Zhenglun Kong*</b>, Peiyan Dong*, Xin Meng, Pinrui Yu, Yanyue Xie, Yifan Gong, Geng Yuan, Fei Sun, Hao Tang, Yanzhi Wang
              <br>
              <em><b>[NeurIPS 2023]</b></em> <i>Advances in Neural Information Processing Systems</i>
              <br>
              <a href="https://openreview.net/pdf?id=3Cj67k38st"><font color="red">PDF</font></a> / 
              <a href="https://github.com/PeiyanFlying/HotBEV"><font color="red">Code</font></a>
              <p></p>
              <p> We present a hardware-oriented transformer-based framework for 3D detection tasks, which achieves higher detection precision and remarkable speedup across high-end and low-end GPUs.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ICML2023.png" alt="3DSP" width="180" height="135" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP" href="https://openreview.net/pdf?id=5VdcSxrlTK">
                <papertitle>SpeedDETR: Speed-aware Transformers for End-to-end Object Detection</papertitle>
              </a>
              <br>
              Peiyan Dong*, <b>Zhenglun Kong*</b>, Xin Meng, Peng Zhang, Hao Tang, Yanzhi Wang, Chih-Hsien Chou
              <br>
              <em><b>[ICML 2023]</b></em> <i>International Conference on Machine Learning</i>
              <br>
              <a href="https://openreview.net/pdf?id=5VdcSxrlTK"><font color="red">PDF</font></a> / 
              <a href="https://github.com/PeiyanFlying/SpeedDETR"><font color="red">Code</font></a>
              <p></p>
              <p> We propose a novel speed-aware transformer for end-to-end object detectors, achieving high-speed inference on multiple devices.</p>
            </td>
          </tr>
          <tr>

          <tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/AAAI2023.png" alt="3DSP" width="180" height="135" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP" href="https://arxiv.org/pdf/2211.10801.pdf">
                <papertitle>Peeling the Onion: Hierarchical Reduction of Data Redundancy for Efficient Vision Transformer Training</papertitle>
              </a>
              <br>
              <b>Zhenglun Kong</b>, Haoyu Ma, Geng Yuan, Mengshu Sun, Yanyue Xie, Peiyan Dong,  Yanzhi Wang, et al.
              <br>
              <em><b>[AAAI 2023 <font color="red">Oral</font>]</b></em> <i>The Thirty-Seventh AAAI Conference on Artificial Intelligence</i>
              <br>
              <a href="https://arxiv.org/pdf/2211.10801.pdf"><font color="red">PDF</font></a> / 
              <a href="https://github.com/ZLKong/Tri-Level-ViT"><font color="red">Code</font></a>
              <p></p>
              <p> We introduce sparsity into data and propose an end-to-end efficient training framework to accelerate ViT training and inference.  </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/IJCAI2023.png" alt="3DSP" width="180" height="135" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP" href="https://www.ijcai.org/proceedings/2023/0153.pdf">
                <papertitle>Data Level Lottery Ticket Hypothesis for Vision Transformers</papertitle>
              </a>
              <br>
              Xuan Shen, <b>Zhenglun Kong</b>, Minghai Qin, Peiyan Dong, Geng Yuan, Xin Meng, Hao Tang, Xiaolong Ma, Yanzhi Wang
              <br>
              <em><b>[IJCAI 2023 <font color="red">Oral</font>]</b></em> <i>The 32nd International Joint Conference on Artificial Intelligence</i>
              <br>
              <a href="https://www.ijcai.org/proceedings/2023/0153.pdf"><font color="red">PDF</font></a> / 
              <a href="https://github.com/shawnricecake/vit-lottery-ticket-input"><font color="red">Code</font></a>
              <p></p>
              <p> We generalize the LTH for ViTs to input data consisting of image patches inspired by the input dependence of ViTs. That is, there exists a subset of input image patches such that a ViT can be trained from scratch by using only this subset of patches and achieve similar accuracy to the ViTs trained by using all image patches.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ECCV2022.png" alt="3DSP" width="180" height="135" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP" href="https://arxiv.org/pdf/2112.13890.pdf">
                <papertitle>SPViT: Enabling Faster Vision Transformers via Latency-aware Soft Token Pruning</papertitle>
              </a>
              <br>
              <b>Zhenglun Kong</b>, Peiyan Dong, Xiaolong Ma, Xin Meng, Mengshu Sun, Wei Niu, Xuan Shen, Geng Yuan, Bin Ren, Minghai Qin, Hao Tang, Yanzhi Wang
              <br>
              <em><b>[ECCV 2022]</b></em> <i>European Conference on Computer Vision</i>
              <br>
              <b><font color="red">CVPRW 2022 Spotlight</font></b>
              <br>
              <a href="https://arxiv.org/pdf/2112.13890.pdf"><font color="red">PDF</font></a> / 
              <a href="https://github.com/PeiyanFlying/SPViT"><font color="red">Code</font></a>
              <p></p>
              <p> We propose a dynamic, latency-aware soft token pruning framework for Vision Transformer. Our framework significantly reduces the computation cost of ViTs while maintaining comparable performance on image classification.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/EMNLP2020.png" alt="3DSP" width="180" height="135" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP" href="https://browse.arxiv.org/pdf/2009.08065.pdf">
                <papertitle>Efficient Transformer-based Large Scale Language Representations using Hardware-friendly Block Structured Pruning</papertitle>
              </a>
              <br>
              Bingbing Li*, <b>Zhenglun Kong*</b>, Tianyun Zhang, Ji Li, Zhengang Li, Hang Liu, Caiwen Ding
              <br>
              <em><b>[EMNLP 2020</b> Findings<b>]</b></b></em> <i>Conference on Empirical Methods in Natural Language Processing</i>
              <br>
              <a href="https://browse.arxiv.org/pdf/2009.08065.pdf"><font color="red">PDF</font></a> / 
              <a href="https://github.com/ZLKong/BERT_demo"><font color="red">Code</font></a>
              <p></p>
              <p>  We propose an efficient transformer-based large-scale language representation using hardware-friendly block structure pruning. We incorporate the reweighted group Lasso into block-structured pruning for optimization.  </p>
            </td>
          </tr>

          <tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/IJCAI2021.png" alt="3DSP" width="180" height="135" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP" href="https://browse.arxiv.org/pdf/2106.00526.pdf">
                <papertitle>A Compression-Compilation Framework for On-mobile Real-time BERT Applications</papertitle>
              </a>
              <br>
              Wei Niu*, <b>Zhenglun Kong*</b>, Geng Yuan, Weiwen Jiang, Jiexiong Guan, Caiwen Ding, Pu Zhao, Sijia Liu, Bin Ren, Yanzhi Wang
              <br>
              <em><b>[IJCAI 2021</b> Demo<b>]</b></em> <i>30th International Joint Conference on Artificial Intelligence</i>
              <br>
              <a href="https://browse.arxiv.org/pdf/2106.00526.pdf"><font color="red">PDF</font></a> 
              <p></p>
              <p> We propose a compression-compilation codesign framework that can guarantee BERT model to meet both resource and real-time specifications of mobile devices. </p>
            </td>
          </tr>

          <tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/MRI.png" alt="3DSP" width="180" height="135" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a id="3DSP" href="https://www.hindawi.com/journals/jhe/2019/2912458/">
                <papertitle>Automatic Tissue Image Segmentation Based on Image Processing and Deep Learning</papertitle>
              </a>
              <br>
              <b>Zhenglun Kong</b>, Ting Li, Junyi Luo, Shengpu Xu
              <br>
              <i>Journal of Healthcare Engineering</i>
              <br>
              <a href="https://www.hindawi.com/journals/jhe/2019/2912458/"><font color="red">PDF</font></a> 
              <p></p>
              <p> We realize automatic image segmentation with convolutional neural network to extract the accurate contours of four tissues: the skull, cerebrospinal fluid (CSF), grey matter (GM), and white matter (WM) on 5 MRI head image datasets. </p>
            </td>
          </tr>
        </table>



        <table
          style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Education</heading>
                <p>
                <ul id="exp" style="list-style:none;">
                  <li style="margin-bottom: 20px;"> <img class="school-logo" src="images/northeastern_logo.png">
                    <div class="school-text"> Northeastern University, Sep 2019 - Present,<br />PhD in Computer Engineering
                      <br /> Advisor: Prof. <a href="https://web.northeastern.edu/yanzhiwang/#_ga=2.59245165.1964588443.1663640196-2055581220.1641240155">Yanzhi Wang</a>
                    </div>
                  </li>
                  <li style="margin-bottom: 20px;"> <img class="school-logo" src="images/northeastern_logo.png">
                    <div class="school-text">Northeastern University, Sep 2017 - May 2019 <br />M.S. in Computer Engineering
                     <br /> Advisor: Prof. <a href="http://www1.ece.neu.edu/~yunfu/">Yun Raymond Fu</a>  
                    </div>
                  <li style="margin-bottom: 20px;"> <img class="school-logo" src="images/Hustseals.png">
                    <div class="school-text" style="width: 80%;">Huazhong University of Science and Technology, China, Sep 2013 - July 2017 <br /> B.E. in Optoelectronic Information Science and Engineering
                     <br /> 
                    </div>
                  </li>
                </ul>
                </p>
              </td>
            </tr>
          </tbody>
        </table>

 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="100%" valign="middle">
            <heading id="Teaching">Teaching Experiences</heading>

        <ul>
          <li>
            <b>Teaching Assistant at Northeastern University</b>

                <ul>
                <li><p>
                EECE 7205 Fundamentals of Computer Engineering, Fall 2021<br>
                Instructor: <a href="https://web.northeastern.edu/xuelin/">Prof. Xue Lin</a> &nbsp&nbsp 
                </p>

                <li><p>
                EECE 5552 Assistive Robotics, Fall 2019<br>
                Instructor: <a href="https://coe.northeastern.edu/people/ramezani-alireza/">Prof. Alireza Ramezani</a> &nbsp&nbsp
                </p>

                <li><p>
                EECE 5644 34595 Machine Learning/Pattern Recognition, Spring 2019<br>
                Instructor: <a href="https://coe.northeastern.edu/people/dy-jennifer/">Prof. Jennifer Dy</a> &nbsp&nbsp
                </p>
                </ul>

          </li>
        </ul>

        <ul>
          <li>
            <b>Guest Lecturer</b>
              <ul>
                <li><p>
                    CSC 791&591 Advanced Topics in Efficient Deep Learning <br>
                    North Carolina State University, 2022 Fall<br>
                    Instructor: <a href="https://www.csc.ncsu.edu/people/dxu27">Prof. Dongkuan Xu</a> &nbsp&nbsp
                    </p>
                </li>
              </ul>
          </li>
        </ul>

          </td>
        </tr>
        </table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="100%" valign="middle">
            <heading id="Talks">Professional Talks</heading>

      <ul>
      <li><p>
      Towards Efficient Deep Learning for Practical AI Implementation <br>
      Carnegie Mellon University, Pittsburgh, PA, Jan. 2024.<br>
      </p>
      <li><p>
      The Lottery Ticket Hypothesis for Vision Transformers <br>
      IJCAI, Macao, SAR, Aug. 2023.<br>
      </p>
      <li><p>
      Peeling the Onion: Hierarchical Reduction of Data Redundancy for Efficient Vision Transformer Training<br>
      AAAI, Washington, DC, Feb. 2023.<br>
      </p>
      <li><p>
      Enabling Faster Vision Transformers via Soft Token Pruning (<a href="https://www.emc2-ai.org/aaai-23">link</a>)<br>
      The 8th EMC2 - Energy Efficient Training and Inference of Transformer Based Models, Washington, DC, Feb. 2023.<br>
      </p>
      <li><p>
      Vision Transformer Optimization<br>
      ARM, San Jose, CA, Aug. 2021.<br>
      </p>
      <li><p>
      A Compression-Compilation Framework for On-mobile Real-time BERT Applications.<br>
      IJCAI, Montreal-themed virtual reality, Aug 2021.<br>
      </p>
      <li><p>
      Hardware-friendly Block Structured Pruning for Transformer<br>
      ARM, San Jose, CA, Jun. 2021.<br>
      </p>
      <li><p>
      Compiler-aware Neural Architecture Optimization for Transformer<br>
      Samsung Research America, Mountain View, CA, Oct. 2020.<br>
      </p>
      </ul>
      </td>
    </tr>
    </table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="100%" valign="middle">
            <heading id="Services">Professional Services</heading>

      <ul>
        <li>
          <b>Conference Reviewer:</b>
            <ul>
              <li>
                ICML2022, ECCV2022, NeurIPS2022, AAAI2023, CVPR2023, KDD2023, IJCAI2023, ICML2023, NeurIPS2023, AAAI2023
              </li>
            </ul>
        </li>
      </ul>

      <ul>
        <li>
          <b>Journal Reviewer:</b>
            <ul>
              <li>
                IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)
              </li>
              <li>
                IEEE Transactions on Neural Networks and Learning Systems (TNNLS)
              </li>
              <li>
                IEEE Transactions on Image Processing (TIP)
              </li>
              <li>
                Pattern Recognition
              </li>
              <li>
                Neurocomputing
              </li>
            </ul>
        </li>
      </ul>

      <ul>
        <li>
          <b>Academic Committee Member:</b>
            <ul>
              <li>
                MLNLP
              </li>
            </ul>
        </li>
      </ul>


          </td>
        </tr>
        </table>

<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=UB1hTbf72-xUPbmw8RtkgYz4tBr01tkZrbYe5zRqidk"></script>
      </td>
    </tr>
  </table>
</body>

</html>
